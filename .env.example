# Copy to .env and fill in secrets. NEVER commit your real .env
# Terraform variable overrides (optional)
TF_VAR_prefix=llmobs
TF_VAR_location=eastus
TF_VAR_container_image=ghcr.io/your-org/llm-observability-otel:latest
# TF_VAR_resource_group_name=existing-rg-name

# Application telemetry / identity
APPINSIGHTS_CONNECTION_STRING=InstrumentationKey=YOUR_KEY;IngestionEndpoint=https://YOUR_ENDPOINT
SERVICE_NAME=langgraph-multi-agent
SERVICE_VERSION=0.1.0
DEPLOYMENT_ENV=local

# LLM provider auth (choose one)
# OPENAI_API_KEY=sk-... 
# OPENAI_BASE_URL=https://api.openai.com/v1
# GITHUB_MODELS_API_KEY=ghp_...
# GITHUB_MODELS_BASE_URL=https://models.github.ai/inference/

# Azure OpenAI (alternative). If using Azure OpenAI you normally set a deployment name, not a raw model id.
# The current code can be adapted by setting OPENAI_BASE_URL to the Azure endpoint below.
# AZURE_OPENAI_ENDPOINT=https://your-azure-openai-resource.openai.azure.com
# AZURE_OPENAI_API_KEY=your-azure-openai-key
# AZURE_OPENAI_DEPLOYMENT=gpt-4o-mini  # Your deployment name (NOT the base model name if you renamed it)
# AZURE_OPENAI_API_VERSION=2024-08-01-preview
# Example mapping for current client usage:
#   export OPENAI_API_KEY=$AZURE_OPENAI_API_KEY
#   export OPENAI_BASE_URL="$AZURE_OPENAI_ENDPOINT/openai"  # client will append paths
# (To fully align with Azure OpenAI REST you may need to enhance code to append /deployments/{deployment}/ and api-version query.)

# Model selection
MODEL_NAME=openai/gpt-4o-mini
